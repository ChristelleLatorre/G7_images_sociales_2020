{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Wed Jan 08 10:05:38 2020\n",
    "<br>\n",
    "Group 7\n",
    "<br>\n",
    "@authors: L.D., C.T.D., C.L.\n",
    "\n",
    "<h1>Group 7 - Images sociales<span class=\"tocSkip\"></span>\n",
    "        \n",
    "<br>    \n",
    "<center>Hyperparameters Optimization using talos<center>\n",
    "<img src=\"../README_images/g7_talos.png?raw=true\">\n",
    "    \n",
    "https://github.com/autonomio/talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires the installation of `talos`.\n",
    "\n",
    "`talos` offers a fully automated hyperparameter tuning and model evaluation for Deep Learning models (equivalent to `gridSearch` in Machine Learning). This notebook presents a `talos`-based method we used to find the best hyperparameters in one of our first models.\n",
    "\n",
    "The output is a `CSV` file summarizing for each hyperparameters combination: `round_epochs`, `val_loss`, `val_accuracy`, `loss`, and `accuracy`.\n",
    "The last columns indicates the hyperparameters values (in this notebook, `activation` and `conv_dropout`).\n",
    "The resulting `CSV` can be found in the same folder as this notebook.\n",
    "\n",
    "Because of time limitations, we did not use this method on the models we trained during the project. However, with more time, it could be interesting to use it to increase the models' accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "To ensure a proper functioning of this code file, `python 3.6` or later version is required.\n",
    "\n",
    "Run the following command if you don't have `talos` installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import talos as ta\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Conv2D, MaxPooling2D, InputLayer, ReLU, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras 2.3.1\n",
      "tensorflow 1.13.1\n",
      "PIL 6.2.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p keras,tensorflow,PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = './../'\n",
    "scrap_path = project_path + 'Scraping/'\n",
    "airliners_path = scrap_path + 'Airliners/data/'\n",
    "\n",
    "size = (256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading(path: str, shape: tuple, split_limit: float=0.7) -> (np.array, np.array):\n",
    "    \"\"\"Splits data into train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        path: path to images\n",
    "        shape: desired shape (height, width)\n",
    "        split_limit: % of images to use as train set\n",
    "        \n",
    "    Out:\n",
    "        data_train: train images in numpy array format\n",
    "        data_test: test images in numpy array format\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data_train = []\n",
    "    data_test = []\n",
    "    \n",
    "    # For each image\n",
    "    for i in tqdm(os.listdir(path)):\n",
    "        if i[-3:] == 'jpg':\n",
    "            rand = np.random.random()\n",
    "            if rand <= split_limit:\n",
    "                img = Image.open(path + i)\n",
    "                data_train.append(\n",
    "                    np.mean(np.array(img.resize(shape, Image.BILINEAR)), axis=2))\n",
    "\n",
    "            else:\n",
    "                img = Image.open(path + i)\n",
    "                data_test.append(\n",
    "                    np.mean(np.array(img.resize(shape, Image.BILINEAR)), axis=2))\n",
    "                \n",
    "    data_train = np.array(data_train)\n",
    "    data_test = np.array(data_test)\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "def create_model(data_train, ytrain, data_test, ytest, nb_classes: int, para: dict,\n",
    "                 input_shape: tuple, nb_layers: int=5, nb_neurons: int=4,\n",
    "                 kernel=(3, 3), pool=(2, 2)):\n",
    "    \"\"\"Create, compile, and fit a basic CNN.\n",
    "\n",
    "    Parameters:\n",
    "        data_train: numpy array with train images\n",
    "        ytrain: numpy array with train labels\n",
    "        data_test: numpy array with test images\n",
    "        ytest: numpy array with test labels\n",
    "        nb_classes: number of classes\n",
    "        para: dict containing hyperparameters (keys) and values (values) to scan\n",
    "              e.g.: {'conv_dropout': [0.1, 0.2]}\n",
    "        input_shape: shape of images (height, width, 1 if greyscale else 3)\n",
    "        nb_layers: number of layers\n",
    "        nb_neurons: number of neurons\n",
    "        kernel: kernel size for Conv2D layers\n",
    "        pool: pool size for MaxPooling2D layers\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    conv_dropout = para['conv_dropout']\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        model.add(Conv2D(2**(nb_neurons + i), kernel_size=kernel,\n",
    "                         activation=para['activation']))\n",
    "        model.add(Dropout(conv_dropout))\n",
    "\n",
    "        model.add(Conv2D(2**(nb_neurons + i), kernel_size=kernel,\n",
    "                         activation=para['activation']))\n",
    "        model.add(Dropout(conv_dropout))\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=pool))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "    out = model.fit(\n",
    "        data_train, ytrain, epochs=10, batch_size=32,\n",
    "        verbose=0,\n",
    "        validation_data=[data_test, ytest])\n",
    "\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and pre-process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592c376616ba45e1995e70f8f5ec448c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592c376616ba45e1995e70f8f5ec448c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079b670f6ef74f19a7ebdb348f720d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbdccc7ea78473eb7d0085896070c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44526510697441e89f738c65458dcc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16dd8beeeba405699b8500a837bf387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450206bf0f574408be58df496c9d2de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230cb19485284d868606b3901da26613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Airbus\n",
    "A320_train, A320_test = reading(airliners_path + 'Airbus/A320/', size)\n",
    "A330_train, A330_test = reading(airliners_path + 'Airbus/A330/', size)\n",
    "A350_train, A350_test = reading(airliners_path + 'Airbus/A350/', size)\n",
    "A380_train, A380_test = reading(airliners_path + 'Airbus/A380/', size)\n",
    "\n",
    "# Boeing\n",
    "Boeing737_train, Boeing737_test = reading(\n",
    "    airliners_path + 'Boeing/737/', size)\n",
    "Boeing747_train, Boeing747_test = reading(\n",
    "    airliners_path + 'Boeing/747/', size\n",
    "Boeing767_train, Boeing767_test=reading(\n",
    "    airliners_path + 'Boeing/767/', size)\n",
    "Boeing777_train, Boeing777_test=reading(\n",
    "    airliners_path + 'Boeing/777/', size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make labels vectors\n",
    "# Train\n",
    "yBoeing_train = np.repeat('Boeing', len(\n",
    "    Boeing777_train) + len(Boeing767_train) + len(Boeing747_train) + len(Boeing737_train))\n",
    "yAirbus_train = np.repeat('Airbus', len(\n",
    "    A380_train) + len(A350_train) + len(A330_train) + len(A320_train))\n",
    "\n",
    "# Test\n",
    "yBoeing_test = np.repeat('Boeing', len(\n",
    "    Boeing777_test) + len(Boeing767_test) + len(Boeing747_test) + len(Boeing737_test))\n",
    "yAirbus_test = np.repeat('Airbus', len(A380_test) +\n",
    "                         len(A350_test) + len(A330_test) + len(A320_test))\n",
    "\n",
    "# Concatenate and get dummies\n",
    "ytrain = np.concatenate((yAirbus_train, yBoeing_train))\n",
    "ytrain = pd.get_dummies(pd.Series(ytrain)).values\n",
    "\n",
    "ytest = np.concatenate((yAirbus_test, yBoeing_test))\n",
    "ytest = pd.get_dummies(pd.Series(ytest)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4815, 256, 256, 1)\n",
       "(2378, 256, 256, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate and reshape train and test sets\n",
    "data_train = np.concatenate((A320_train, A330_train, A350_train, A380_train,\n",
    "                             Boeing777_train, Boeing767_train, Boeing747_train, Boeing737_train))\n",
    "\n",
    "data_test = np.concatenate((A320_test, A330_test, A350_test, A380_test,\n",
    "                            Boeing777_test, Boeing767_test, Boeing747_test, Boeing737_test))\n",
    "\n",
    "data_train = np.reshape(data_train, (data_train.shape[0], size[0], size[1], 1))\n",
    "\n",
    "data_test = np.reshape(data_test, (data_test.shape[0], size[0], size[1], 1))\n",
    "\n",
    "print(data_train.shape, data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "arr = np.arange(ytrain.shape[0])\n",
    "np.random.shuffle(arr)\n",
    "\n",
    "ytrain = ytrain[arr]\n",
    "data_train = data_train[arr]\n",
    "\n",
    "arr = np.arange(ytest.shape[0])\n",
    "np.random.shuffle(arr)\n",
    "\n",
    "ytest = ytest[arr]\n",
    "data_test = data_test[arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = ytrain.shape[1]\n",
    "input_shape = (size[0], size[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply hyperparameters scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]\n",
      "\n",
      " 25%|████████████████████                                                            | 1/4 [50:17<2:30:52, 3017.56s/it]\n",
      "\n",
      " 50%|███████████████████████████████████████                                       | 2/4 [2:40:32<2:16:33, 4096.64s/it]\n",
      "\n",
      " 75%|█████████████████████████████████████████████████████████                   | 3/4 [15:06:15<4:31:30, 16290.73s/it]\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4/4 [17:06:24<00:00, 15396.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# Parameters to scan\n",
    "para = {'activation': ['relu', 'softmax'],\n",
    "        'conv_dropout': [0.1, 0.2]}\n",
    "\n",
    "# Compute hyperparameters optimization and save to csv\n",
    "scan_results = ta.Scan(data_train, ytrain, para, create_model(data_train, ytrain, data_test, ytest, nb_classes, para,\n",
    "                                                              input_shape, nb_layers=5, nb_neurons=4, kernel=(3, 3), pool=(2, 2)),\n",
    "                       'talos_V0', x_val=data_test, y_val=ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
