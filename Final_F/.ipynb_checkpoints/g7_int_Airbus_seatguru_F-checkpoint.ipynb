{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Fri Jan 13 14:20:37 2020\n",
    "<br>\n",
    "Group 7\n",
    "<br>\n",
    "@authors: E.G., C.L.\n",
    "<h1>Group 7 - Images sociales<span class=\"tocSkip\"></span>\n",
    "        \n",
    "<br>    \n",
    "<center>Airbus Interiors - aircraft types model - SeatGuru<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Environment\" data-toc-modified-id=\"Environment-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Functions</a></span></li></ul></li><li><span><a href=\"#Pre-processing\" data-toc-modified-id=\"Pre-processing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pre-processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-directories\" data-toc-modified-id=\"Create-directories-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create directories</a></span></li><li><span><a href=\"#Train-test-split-and-read-images\" data-toc-modified-id=\"Train-test-split-and-read-images-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Train-test split and read images</a></span></li></ul></li><li><span><a href=\"#Build,-save,-and-train-model\" data-toc-modified-id=\"Build,-save,-and-train-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build, save, and train model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook trains a model to predict Airbus aircraft types on images representing an aircraft interior. The images used for training come from Seatguru social media.\n",
    "\n",
    "**Pre-processing**<span class=\"tocSkip\"></span><br>\n",
    "By reading and filtering the CSV file that contains the labels (Int, Ext, Ext-Int, Meal), we get the list of Interior labelled images. Then, the images are copied to directories (one per desired aircraft type), and split into train and test sets. If the data augmentation option is set to `True`, the train set will be enriched with new images (obtained by cropping / (de)zooming / rotating / flipping existing images).\n",
    "\n",
    "**Model**<span class=\"tocSkip\"></span><br>\n",
    "We get weights from VGG16 pre-trained model, and add some layers (Conv2D, ReLU, MaxPooling2D, Flatten, and Dense) to predict the target classes (e.g.: 5 classes could be A320, A321, A330, A350, and A380).\n",
    "\n",
    "**Out**<span class=\"tocSkip\"></span><br>\n",
    "After training, a folder is created in `Models` repository, containing the model in `h5` format, along with the corresponding labels stored in a `pickle` dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "To ensure a proper functioning of this code file, `python 3.6` or later version is required.\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T15:19:11.728311Z",
     "start_time": "2020-01-09T15:19:06.196795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, InputLayer, ReLU, AveragePooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras 2.3.1\n",
      "tensorflow 1.13.1\n",
      "PIL 6.2.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p keras,tensorflow,PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = './../'\n",
    "seatguru_path = project_path + 'Interpromo2020/All Data/ANALYSE IMAGE/IMG SEATGURU/'\n",
    "stats_path = project_path + 'ImagesStats/'\n",
    "\n",
    "model_name = 'Int_Airbus_Seatguru_F'\n",
    "\n",
    "# Classes to predict\n",
    "airbus_planes = ['A320', 'A321', 'A330', 'A350', 'A380']\n",
    "nb_types = len(airbus_planes)\n",
    "\n",
    "# Images parameters\n",
    "size = (224, 224)\n",
    "greys = False\n",
    "apply_data_augmentation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run g7_functions_for_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "## Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../G7_SEATGURU/Int/Airbus/data_train ./../G7_SEATGURU/Int/Airbus/data_test\n",
      "./../G7_SEATGURU/Int/Airbus/A320\n",
      "A320: 166 images\n",
      "./../G7_SEATGURU/Int/Airbus/A321\n",
      "A321: 121 images\n",
      "./../G7_SEATGURU/Int/Airbus/A330\n",
      "A330: 257 images\n",
      "./../G7_SEATGURU/Int/Airbus/A350\n",
      "A350: 39 images\n",
      "./../G7_SEATGURU/Int/Airbus/A380\n",
      "A380: 46 images\n"
     ]
    }
   ],
   "source": [
    "# Read SEATGURU annotated CSV\n",
    "df_seat_annot = pd.read_csv(stats_path + 'g7_SEATGURU_annotate.csv', sep=';', engine='python',\n",
    "                            index_col=None, encoding='utf-8')\n",
    "\n",
    "# Paths where to create aircraft types folders\n",
    "crea_path = project_path + 'G7_SEATGURU/Int/Airbus/'\n",
    "new_paths = [crea_path + 'data_train', crea_path + 'data_test']\n",
    "print(new_paths[0], new_paths[1])\n",
    "\n",
    "# Create a directory for each class\n",
    "create_dirs_seatguru_type(df_seat_annot, project_path, seatguru_path, crea_path, aircraft_types=airbus_planes,\n",
    "                          view='Int', man='Airbus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split and read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "split_train_test_seatguru_type(\n",
    "    new_paths=new_paths, path=crea_path, aircraft_types=airbus_planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: use data augmentation to enrich your train set\n",
    "if apply_data_augmentation:\n",
    "    %run g7_data_augmentation.py\n",
    "    data_augmentation(train_path=new_paths[0], shape=size, save_format='jpeg', nb_win=2, coef_gen=2,\n",
    "                      greys=False, rotation_range=20, shear_range=.2, zoom_range=.15, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T15:26:28.959798Z",
     "start_time": "2020-01-09T15:26:19.480789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 438 images belonging to 5 classes.\n",
      "Found 191 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image data generator\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = datagen.flow_from_directory(new_paths[0],\n",
    "                                              target_size=size,\n",
    "                                              color_mode='rgb',\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='categorical',\n",
    "                                              shuffle=True)\n",
    "\n",
    "test_generator = datagen.flow_from_directory(new_paths[1],\n",
    "                                             target_size=size,\n",
    "                                             color_mode='rgb',\n",
    "                                             batch_size=32,\n",
    "                                             class_mode='categorical',\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build, save, and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T09:22:00.420098Z",
     "start_time": "2020-01-10T09:21:59.487724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 5, 5, 256)         1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "re_lu_15 (ReLU)              (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "re_lu_16 (ReLU)              (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 17,283,397\n",
      "Trainable params: 2,564,613\n",
      "Non-trainable params: 14,718,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the base pre-trained model\n",
    "base_model = VGG16(weights='imagenet', include_top=False,\n",
    "                   input_shape=(size[0], size[1], 1 if greys else 3))\n",
    "x = base_model.output\n",
    "\n",
    "# Add layers\n",
    "x = Conv2D(256, kernel_size=(3, 3))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "x = Conv2D(256, kernel_size=(3, 3))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)  # vector\n",
    "\n",
    "x = Dense(1024)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "x = Dense(512)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "# Last layer used to predict our classes\n",
    "# Dense has the same number of neurons as the number of classes to predict\n",
    "predictions = Dense(nb_types, activation='softmax')(x)\n",
    "\n",
    "# Model to be trained\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Don't retrain pre-trained layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model (should be done after setting layers to non-trainable)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy',  # chosen metric\n",
    "                              patience=2,  # number of epochs\n",
    "                              verbose=1,\n",
    "                              factor=0.5,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T09:46:07.626778Z",
     "start_time": "2020-01-10T09:27:13.063490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13/13 [==============================] - 98s 8s/step - loss: 1.7854 - accuracy: 0.3719 - val_loss: 5.2002 - val_accuracy: 0.1885\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 98s 8s/step - loss: 0.6328 - accuracy: 0.7783 - val_loss: 6.2414 - val_accuracy: 0.2827\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 97s 7s/step - loss: 0.2008 - accuracy: 0.9409 - val_loss: 4.1186 - val_accuracy: 0.2251\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 99s 8s/step - loss: 0.0659 - accuracy: 0.9904 - val_loss: 4.2839 - val_accuracy: 0.4241\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 107s 8s/step - loss: 0.0475 - accuracy: 0.9951 - val_loss: 2.2106 - val_accuracy: 0.3717\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 181s 14s/step - loss: 0.0484 - accuracy: 0.9924 - val_loss: 1.8997 - val_accuracy: 0.3298\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 186s 14s/step - loss: 0.0293 - accuracy: 0.9952 - val_loss: 2.1444 - val_accuracy: 0.4031\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 180s 14s/step - loss: 0.0176 - accuracy: 0.9949 - val_loss: 2.1792 - val_accuracy: 0.3979\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 192s 15s/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 2.5263 - val_accuracy: 0.3979\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 192s 15s/step - loss: 0.0126 - accuracy: 0.9951 - val_loss: 1.6235 - val_accuracy: 0.3927\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 182s 14s/step - loss: 0.0155 - accuracy: 0.9924 - val_loss: 1.5421 - val_accuracy: 0.3927\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 190s 15s/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 1.8528 - val_accuracy: 0.3979\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 190s 15s/step - loss: 0.0097 - accuracy: 0.9975 - val_loss: 1.6934 - val_accuracy: 0.4188\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 185s 14s/step - loss: 0.0112 - accuracy: 0.9951 - val_loss: 1.6519 - val_accuracy: 0.4084\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 98s 8s/step - loss: 0.0108 - accuracy: 0.9951 - val_loss: 2.3959 - val_accuracy: 0.4241\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 95s 7s/step - loss: 0.0110 - accuracy: 0.9901 - val_loss: 2.3056 - val_accuracy: 0.4241\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 94s 7s/step - loss: 0.0100 - accuracy: 0.9951 - val_loss: 2.3027 - val_accuracy: 0.4241\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 94s 7s/step - loss: 0.0089 - accuracy: 0.9975 - val_loss: 1.4882 - val_accuracy: 0.4241\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 94s 7s/step - loss: 0.0104 - accuracy: 0.9951 - val_loss: 1.8271 - val_accuracy: 0.4241\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 97s 7s/step - loss: 0.0069 - accuracy: 0.9976 - val_loss: 2.1580 - val_accuracy: 0.4241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f6d5b250bd0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "step_size_train = train_generator.n // train_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=step_size_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=test_generator,\n",
    "                    callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and labels\n",
    "os.makedirs(project_path + 'Models/' + model_name + '/', exist_ok=True)\n",
    "save_model_classes(project_path + 'Models/',\n",
    "                   model_name, train_generator, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "294px",
    "left": "1070px",
    "right": "30px",
    "top": "120px",
    "width": "340px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
