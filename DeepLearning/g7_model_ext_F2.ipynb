{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Fri Jan 10 14:42:01 2020\n",
    "<br>\n",
    "Group 7\n",
    "<br>\n",
    "@author: P.S.B.\n",
    "<h1>Group 7 - Images sociales<span class=\"tocSkip\"></span>\n",
    "\n",
    "<br>    \n",
    "<center>Exteriors - aircraft types model<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Environment\" data-toc-modified-id=\"Environment-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Functions</a></span></li></ul></li><li><span><a href=\"#Train-test-split-and-read-images\" data-toc-modified-id=\"Train-test-split-and-read-images-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train-test split and read images</a></span></li><li><span><a href=\"#Build,-save,-and-train-model\" data-toc-modified-id=\"Build,-save,-and-train-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build, save, and train model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook trains a model to predict aircraft types on images representing an aircraft exterior. The images used for training come from Airliners scraping (\"clean\" images of different types of aircraft exteriors).\n",
    "\n",
    "**Pre-processing**<span class=\"tocSkip\"></span><br>\n",
    "Images are split into train and test sets. If the data augmentation option is set to `True`, the train set will be enriched with new images (obtained by cropping / (de)zooming / rotating / flipping existing images).\n",
    "\n",
    "**Model**<span class=\"tocSkip\"></span><br>\n",
    "We get weights from VGG16 pre-trained model, and add some layers (Conv2D, ReLU, MaxPooling2D, Flatten, and Dense) to predict the target classes (Airbus and Boeing types).\n",
    "\n",
    "**Out**<span class=\"tocSkip\"></span><br>\n",
    "After training, a folder is created in `Models` repository, containing the model in `h5` format, along with the corresponding labels stored in a `pickle` dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "To ensure a proper functioning of this code file, `python 3.6` or later version is required.\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T14:11:14.216688Z",
     "start_time": "2020-01-17T14:11:12.542348Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, InputLayer, ReLU, AveragePooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras 2.3.1\n",
      "tensorflow 1.13.1\n",
      "PIL 6.2.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p keras,tensorflow,PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = './../'\n",
    "Airliners_path = project_path + 'Scraping/Airliners/data'\n",
    "new_paths = [project_path + 'Split_Data/Airliners/Train',\n",
    "             project_path + 'Split_Data/Airliners/Test']\n",
    "\n",
    "model_name = 'Ext_F_2'\n",
    "\n",
    "# Classes\n",
    "airbus_planes = ['A320', 'A321', 'A330', 'A350']\n",
    "boeing_planes = ['737', '747', '757', '777']\n",
    "nb_types = len(airbus_planes) + len(boeing_planes)\n",
    "\n",
    "# Images parameters\n",
    "size = (224, 224)\n",
    "greys = False\n",
    "apply_data_augmentation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run g7_functions_for_models.py\n",
    "# %run g7_data_augmentation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split and read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_train_test_airliners(Airliners_path, new_paths,\n",
    "                         airbus_types=airbus_planes, boeing_types=boeing_planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: use data augmentation to enrich your train set\n",
    "if apply_data_augmentation:\n",
    "    %run g7_data_augmentation.py\n",
    "    data_augmentation(train_path=new_paths[0], shape=size, save_format='jpeg', nb_win=2, coef_gen=2,\n",
    "                      greys=False, rotation_range=20, shear_range=.2, zoom_range=.15, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T14:14:37.818654Z",
     "start_time": "2020-01-17T14:14:36.796871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5600 images belonging to 8 classes.\n",
      "Found 2400 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image data generator\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = datagen.flow_from_directory(new_paths[0],\n",
    "                                              target_size=size,\n",
    "                                              color_mode='rgb',\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='categorical',\n",
    "                                              shuffle=True)\n",
    "\n",
    "test_generator = datagen.flow_from_directory(new_paths[1],\n",
    "                                             target_size=size,\n",
    "                                             color_mode='rgb',\n",
    "                                             batch_size=32,\n",
    "                                             class_mode='categorical',\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build, save, and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T14:14:46.103232Z",
     "start_time": "2020-01-17T14:14:45.197578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 4104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 40,940,904\n",
      "Trainable params: 26,223,128\n",
      "Non-trainable params: 14,717,776\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the base pre-trained model\n",
    "base_model = VGG16(weights='imagenet', include_top=False,\n",
    "                   input_shape=(size[0], size[1], 1 if greys else 3))\n",
    "x = base_model.output\n",
    "\n",
    "# Add layers\n",
    "x = Flatten()(x)  # vector\n",
    "\n",
    "x = Dense(1024)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "x = Dense(512)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "# Last layer used to predict our classes\n",
    "# Dense has the same number of neurons as the number of classes to predict\n",
    "x = Dense(nb_types)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "predictions = Softmax()(x)\n",
    "\n",
    "# Model to be trained\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Don't retrain pre-trained layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model (should be done after setting layers to non-trainable)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T13:28:16.480194Z",
     "start_time": "2020-01-17T13:28:13.440805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy',  # chosen metric\n",
    "                              patience=2,  # number of epochs\n",
    "                              verbose=1,\n",
    "                              factor=0.5,\n",
    "                              min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T15:24:33.075413Z",
     "start_time": "2020-01-17T14:14:53.723336Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "175/175 [==============================] - 1292s 7s/step - loss: 1.2279 - accuracy: 0.6064 - val_loss: 0.7389 - val_accuracy: 0.7275\n",
      "Epoch 2/10\n",
      "175/175 [==============================] - 1279s 7s/step - loss: 0.5156 - accuracy: 0.9309 - val_loss: 0.5370 - val_accuracy: 0.7908\n",
      "Epoch 3/10\n",
      "175/175 [==============================] - 2799s 16s/step - loss: 0.3189 - accuracy: 0.9768 - val_loss: 0.5975 - val_accuracy: 0.7987\n",
      "Epoch 4/10\n",
      "175/175 [==============================] - 2056s 12s/step - loss: 0.2402 - accuracy: 0.9862 - val_loss: 0.5833 - val_accuracy: 0.8208\n",
      "Epoch 5/10\n",
      "175/175 [==============================] - 1325s 8s/step - loss: 0.2142 - accuracy: 0.9861 - val_loss: 0.5137 - val_accuracy: 0.8171\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/10\n",
      "175/175 [==============================] - 2396s 14s/step - loss: 0.1605 - accuracy: 0.9925 - val_loss: 0.5471 - val_accuracy: 0.8404\n",
      "Epoch 7/10\n",
      "175/175 [==============================] - 2564s 15s/step - loss: 0.1460 - accuracy: 0.9920 - val_loss: 0.5688 - val_accuracy: 0.8529\n",
      "Epoch 8/10\n",
      "175/175 [==============================] - 3078s 18s/step - loss: 0.1153 - accuracy: 0.9973 - val_loss: 0.7000 - val_accuracy: 0.8571\n",
      "Epoch 9/10\n",
      "175/175 [==============================] - 3980s 23s/step - loss: 0.1083 - accuracy: 0.9948 - val_loss: 0.6044 - val_accuracy: 0.8296\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/10\n",
      "175/175 [==============================] - 2971s 17s/step - loss: 0.0954 - accuracy: 0.9971 - val_loss: 0.3194 - val_accuracy: 0.8479\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f55da9ee050>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 128\n",
    "step_size_train = train_generator.n // train_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=step_size_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=test_generator,\n",
    "                    callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and labels\n",
    "os.makedirs(project_path + 'Models/' + model_name + '/', exist_ok=True)\n",
    "save_model_classes(project_path + 'Models/',\n",
    "                   model_name, train_generator, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "294px",
    "left": "1070px",
    "right": "30px",
    "top": "120px",
    "width": "340px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
