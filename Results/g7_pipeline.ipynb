{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created on: Tue Jan 14 09:44:36 2020\n",
    "<br>\n",
    "Group 7\n",
    "<br>\n",
    "@authors: V.B., E.G.\n",
    "\n",
    "<h1>Group 7 - Images sociales<span class=\"tocSkip\"></span>\n",
    "    \n",
    "<br>  \n",
    "<center>Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook runs all our models on images folders. Before launching the pipeline, you just have to set `social_net`: name of the desired social network, and make sure you have the corresponding folder filled with images in your `data_path`. Specify `insta_hashtag`, if need be.\n",
    "\n",
    "\n",
    "For the moment you have 2 folders for Seatguru and Instagram. The latter contains 4 subfolders for the following hashtags: airbus, aircraftinterior, aircraftseat, and boeing. You can add new images in any folder and relaunch the pipeline, or create folders for new hashtags and/or social media.\n",
    "\n",
    "\n",
    "**Out**<span class=\"tocSkip\"></span><br>\n",
    "After a pipeline run, you will find CSV files containing predictions in your `Results` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "To ensure a proper functioning of this code file, `python 3.6` or later version is required.\n",
    "\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T15:19:11.728311Z",
     "start_time": "2020-01-09T15:19:06.196795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras import models\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import datetime\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras 2.3.1\n",
      "tensorflow 1.13.1\n",
      "PIL 6.2.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p keras,tensorflow,PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = './../'\n",
    "models_path = project_path + 'Models_F/'\n",
    "data_path = 'Interpromo2020/All Data/ANALYSE IMAGE/'\n",
    "path_out = './'\n",
    "\n",
    "# Choose social network: SEATGURU, INSTAGRAM\n",
    "social_net = 'INSTAGRAM'\n",
    "insta_hashtag = 'boeing'  # if social_net == 'INSTAGRAM'\n",
    "\n",
    "# Choose images parameters\n",
    "size = (224, 224)\n",
    "greys = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_model(path_mod: str, mod_name: str):\n",
    "    \"\"\"Loads a model\n",
    "\n",
    "    Parameters:\n",
    "        path_mod: path to models folders\n",
    "        mod_name: model name\n",
    "\n",
    "    Out:\n",
    "        model: model in h5 format\n",
    "        dic_class: dict with classes labels (keys), and correponding integers returned by the model (values)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.load_model(path_mod + mod_name + '/' + 'model_' + mod_name + '.h5')\n",
    "    with open(path_mod + mod_name + '/' + 'model_' + mod_name + '.pkl', \"rb\") as f:\n",
    "        dic_class = pickle.load(f)\n",
    "        \n",
    "    return model, dic_class\n",
    "\n",
    "\n",
    "def read_img(img_path: str, size: tuple, greys: bool = False) -> np.array:\n",
    "    \"\"\"Read and convert an image to numpy array.\n",
    "\n",
    "    Parameters:\n",
    "        img_path: path to desired image\n",
    "        size: tuple in (width, height) format\n",
    "        greys: False for colour, else True\n",
    "\n",
    "    Out:\n",
    "        img_arr: image in numpy array format\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    img_img = Image.open(img_path)\n",
    "    img_arr = np.array(img_img.resize(size))\n",
    "    img_arr = preprocess_input(img_arr.reshape(\n",
    "        1, size[0], size[1], 1 if greys else 3))\n",
    "\n",
    "    return img_arr\n",
    "\n",
    "\n",
    "def predict_from_model(img_arr: np.array, models_path: str, model_name: str) -> (list, list):\n",
    "    \"\"\"Apply given model to given set of images.\n",
    "\n",
    "    Parameters:\n",
    "        img_arr: set of images in numpy array format\n",
    "        models_path: path to models\n",
    "        model_name: name of the model to apply\n",
    "\n",
    "    Out:\n",
    "       labels: a list of predicted labels (the predicted label is the one with the highest probability)\n",
    "       proba_labels: probability of predicted label\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model and predict\n",
    "    model, dic_class = load_files_model(\n",
    "        path_mod=models_path, mod_name=model_name)\n",
    "    print(dic_class)\n",
    "    preds = model.predict(img_arr)\n",
    "    labels = [np.argmax(preds[k]) for k in range(len(preds))]\n",
    "    proba_labels = [np.max(preds[k]) for k in range(len(preds))]\n",
    "    labels = [list(dic_class.keys())[list(dic_class.values()).index(lab)]\n",
    "              for lab in labels]\n",
    "\n",
    "    del model, dic_class  # remove model and dict from environment\n",
    "\n",
    "    return labels, proba_labels\n",
    "\n",
    "\n",
    "def predict_save(df: pd.DataFrame, all_imgs_arr: np.array, filter_: list, models_path: str,\n",
    "                 model_name: str, to_fill: str) -> (pd.DataFrame, int):\n",
    "    \"\"\"Applies given model to given set of images.\n",
    "\n",
    "    Parameters:\n",
    "        df: DataFrame aimed at containing prediction results\n",
    "            Lines: images; columns: items to predict\n",
    "\n",
    "        all_imgs_arr: numpy array containing all images\n",
    "        filter_: explicits the filter for the images on which the model will be applied.\n",
    "                 Key: column name in the DataFrame; value: value to filter in this column.\n",
    "                 e.g. {'view': 'Ext'} \n",
    "\n",
    "        models_path: path to models\n",
    "        model_name: name of the model to apply\n",
    "        to_fill: name of the df column to fill with results ('type', 'manufacturer')\n",
    "\n",
    "    Out:\n",
    "       df: DataFrame filled with predicted label and probability\n",
    "       i: indexes of filtered images (might be useful if you want to apply further\n",
    "          transformation to these lines in the DataFrame)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    imgs = all_imgs_arr\n",
    "    ix = []\n",
    "\n",
    "    # Apply filter(s)\n",
    "    for k in filter_.keys():\n",
    "        ix.append(df[df[k] == filter_[k]].index.tolist())\n",
    "    ind = ix[0]\n",
    "    if len(ix) == 2:\n",
    "        ind = list(set(ix[0]).intersection(set(ix[1])))\n",
    "    imgs = imgs[ind]\n",
    "\n",
    "    # Get labels with probabilities\n",
    "    labels, proba_labels = predict_from_model(img_arr=imgs, models_path=models_path,\n",
    "                                              model_name=model_name)\n",
    "\n",
    "    # Fill DataFrame\n",
    "    df[to_fill].loc[ind] = labels\n",
    "    df[to_fill + '_proba'].loc[ind] = proba_labels\n",
    "\n",
    "    return df, ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read images to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_s = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "if social_net == 'SEATGURU':\n",
    "    folder = 'IMG ' + social_net + '/'\n",
    "\n",
    "elif social_net == 'INSTAGRAM':\n",
    "    folder = social_net + '/' + insta_hashtag + '/'\n",
    "\n",
    "path_pred = project_path + data_path + folder\n",
    "imgs_names = os.listdir(path_pred)\n",
    "imgs_names = [img for img in imgs_names if '.jpg' in img]\n",
    "\n",
    "# Init results DataFrame\n",
    "df = pd.DataFrame(columns=['img',\n",
    "                           'view',\n",
    "                           'view_proba',\n",
    "                           'manufacturer',\n",
    "                           'manufacturer_proba',\n",
    "                           'type',\n",
    "                           'type_proba'])\n",
    "df['img'] = imgs_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1934, 224, 224, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all images, convert to array\n",
    "all_imgs_arr = np.array([read_img(path_pred + imgs_names[k], size=(size[0], size[1]), greys=greys)\n",
    "                         for k in range(len(imgs_names))])\n",
    "\n",
    "# Reshape for prediction\n",
    "all_imgs_arr = all_imgs_arr.reshape(\n",
    "    len(imgs_names), size[0], size[1], 1 if greys else 3)\n",
    "all_imgs_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ext': 0, 'Ext_Int': 1, 'Int': 2, 'Meal': 3}\n"
     ]
    }
   ],
   "source": [
    "labels, proba_labels = predict_from_model(img_arr=all_imgs_arr, models_path=models_path,\n",
    "                                          model_name='View')\n",
    "\n",
    "# Fill DataFrame with View labels\n",
    "df['view'] = labels\n",
    "df['view_proba'] = proba_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manufacturer and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'737': 0, '747': 1, '757': 2, '777': 3, '787': 4, 'A320': 5, 'A321': 6, 'A330': 7, 'A340': 8, 'A350': 9, 'A380': 10}\n"
     ]
    }
   ],
   "source": [
    "# If View == 'Ext', predict aircraft type and fill DataFrame\n",
    "filter_ = dict({'view': 'Ext'})\n",
    "df, ind = predict_save(df, all_imgs_arr, filter_,\n",
    "                       models_path, model_name='Ext_typ', to_fill='type')\n",
    "\n",
    "# Deduce manufacturer from aircraft type\n",
    "df['manufacturer'].loc[ind] = [\n",
    "    'Airbus' if 'A' in typ else 'Boeing' for typ in df['type'].loc[ind]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manufacturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Airbus': 0, 'Boeing': 1}\n"
     ]
    }
   ],
   "source": [
    "# If View == 'Int', predict manufacturer\n",
    "filter_ = dict({'view': 'Int'})\n",
    "df, ind = predict_save(df, all_imgs_arr, filter_, models_path,\n",
    "                       model_name='Int_man', to_fill='manufacturer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airbus and Boeing types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A320': 0, 'A321': 1, 'A330': 2, 'A350': 3, 'A380': 4}\n",
      "{'737': 0, '747': 1, '757': 2, '777': 3}\n"
     ]
    }
   ],
   "source": [
    "# If View == 'Int' and manufacturer == 'Airbus', predict type\n",
    "filter_ = dict({'view': 'Int',\n",
    "                'manufacturer': 'Airbus'})\n",
    "df, ind = predict_save(df, all_imgs_arr, filter_, models_path,\n",
    "                       model_name='Int_Airbus', to_fill='type')\n",
    "\n",
    "# If View == 'Int' and manufacturer == 'Boeing', predict type\n",
    "filter_ = dict({'view': 'Int',\n",
    "                'manufacturer': 'Boeing'})\n",
    "df, ind = predict_save(df, all_imgs_arr, filter_, models_path,\n",
    "                       model_name='Int_Boeing', to_fill='type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:11:00.246725\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now() - time_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save\n",
    "if social_net != 'INSTAGRAM':\n",
    "    path_save = path_out + '/g7_pred_' + social_net + '_4.csv'\n",
    "\n",
    "else:\n",
    "    path_save = path_out + '/g7_pred_' + social_net + '_' + insta_hashtag + '_4.csv'\n",
    "\n",
    "# Save\n",
    "df.to_csv(path_or_buf=path_save, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "294px",
    "left": "1070px",
    "right": "30px",
    "top": "120px",
    "width": "340px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
